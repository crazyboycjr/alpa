[training_spec]
avg_batch_size_per_device = 1024
num_iters = 20

[[cluster_spec]]
num_hosts = 1
num_devices_per_host = 8
transport = "NVLink"

[[cluster_spec]]
num_hosts = 2
num_devices_per_host = 8
transport = "EFAx4"

[[parallel_spec]]
description = "data parallel"
num_micro_batches = [1]
"(pp, dp, op)" = [1, 8, 1]
physical_mesh_shape = [1, 8]
auto_sharding_option = { 'force_batch_dim_to_mesh_dim' = 0 }
num_auto_layers = 1
remat_layer = false

[[parallel_spec]]
description = "data parallel"
num_micro_batches = [1]
"(pp, dp, op)" = [1, 16, 1]
physical_mesh_shape = [2, 8]
auto_sharding_option = { 'force_batch_dim_to_mesh_dim' = 0 }
num_auto_layers = 1
remat_layer = false

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = ["ATTENTION", "LINEAR", "ATTENTION", "DOT"]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT",], [ "DOT", "LINEAR", "LINEAR", "LINEAR", "LINEAR", "LINEAR", "LINEAR",], [ "DOT", "ATTENTION", "ATTENTION",], [ "ATTENTION",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "DOT", "ATTENTION", "ATTENTION", "ATTENTION",], [ "DOT", "DOT",], [ "DOT", "ATTENTION",], [ "DOT", "DOT", "DOT",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "LINEAR", "LINEAR", "LINEAR", "LINEAR", "LINEAR",], [ "DOT", "LINEAR",], [ "DOT", "DOT", "ATTENTION",], [ "ATTENTION",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "DOT", "DOT", "DOT", "DOT", "LINEAR",], [ "DOT", "ATTENTION",], [ "ATTENTION",], [ "DOT", "DOT", "LINEAR",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "DOT", "DOT", "DOT", "LINEAR", "LINEAR",], [ "DOT", "DOT", "DOT",], [ "DOT",], [ "DOT", "DOT",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT",], [ "LINEAR", "LINEAR", "ATTENTION", "ATTENTION", "ATTENTION", "ATTENTION",], [ "DOT", "ATTENTION",], [ "DOT", "LINEAR", "LINEAR",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "LINEAR", "LINEAR", "ATTENTION",], [ "DOT",], [ "DOT", "DOT", "DOT", "LINEAR", "LINEAR", "ATTENTION",], [ "DOT",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "DOT", "DOT", "LINEAR", "LINEAR", "LINEAR",], [ "DOT", "ATTENTION", "ATTENTION",], [ "DOT",], [ "DOT", "LINEAR",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT", "LINEAR",], [ "DOT", "DOT", "DOT", "DOT", "LINEAR",], [ "DOT", "DOT", "LINEAR",], [ "DOT", "ATTENTION",],]

[[model]]
num_features = 512
emb_dim = 160
output_per_emb = 50
num_zhen_layers = 4
tokens = [ [ "DOT",], [ "DOT", "DOT", "DOT", "DOT", "DOT",], [ "DOT", "DOT", "DOT", "LINEAR", "LINEAR",], [ "DOT",],]