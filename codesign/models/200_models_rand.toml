[training_spec]
avg_batch_size_per_device = 1024
num_iters = 20

[[cluster_spec]]
num_hosts = 1
num_devices_per_host = 8
transport = "NVLink"

[[cluster_spec]]
num_hosts = 2
num_devices_per_host = 8
transport = "EFAx4"

[[parallel_spec]]
description = "data parallel"
num_micro_batches = [1]
"(pp, dp, op)" = [1, 8, 1]
physical_mesh_shape = [1, 8]
auto_sharding_option = { 'force_batch_dim_to_mesh_dim' = 0 }
num_auto_layers = 1
remat_layer = false

[[parallel_spec]]
description = "data parallel"
num_micro_batches = [1]
"(pp, dp, op)" = [1, 16, 1]
physical_mesh_shape = [2, 8]
auto_sharding_option = { 'force_batch_dim_to_mesh_dim' = 0 }
num_auto_layers = 1
remat_layer = false
